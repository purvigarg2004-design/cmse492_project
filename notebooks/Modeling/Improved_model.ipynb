{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9949a06",
   "metadata": {},
   "source": [
    "# B5. Improved Models for \"Will it Rain Tomorrow in Basel?\"\n",
    "\n",
    "This notebook loads the processed Basel feature table from `data/processed/`,\n",
    "uses a time-aware train/test split, and fits stronger models:\n",
    "\n",
    "- Tuned logistic regression (with scaling)\n",
    "- Random forest (nonlinear benchmark)\n",
    "\n",
    "The goal is to see how much we can improve over the simple baseline logistic\n",
    "regression from the B3 notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ce3be6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ee10a3",
   "metadata": {},
   "source": [
    "## 1. Load Processed Basel Feature Table\n",
    "\n",
    "### 1.1 Read `basel_rain_features.csv` and enforce time order\n",
    "\n",
    "Here I load the Basel-only feature table created in the preprocessing notebook.\n",
    "I convert `DATE` back to a datetime type and sort the rows chronologically so\n",
    "that earlier days come first and later days come last.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "831a84ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded processed: (3653, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RainTomorrow</th>\n",
       "      <th>BASEL_pressure</th>\n",
       "      <th>BASEL_humidity</th>\n",
       "      <th>BASEL_temp_mean</th>\n",
       "      <th>BASEL_sunshine</th>\n",
       "      <th>BASEL_pressure_lag1</th>\n",
       "      <th>BASEL_humidity_lag1</th>\n",
       "      <th>BASEL_temp_mean_lag1</th>\n",
       "      <th>BASEL_sunshine_lag1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0318</td>\n",
       "      <td>0.87</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0286</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0314</td>\n",
       "      <td>0.81</td>\n",
       "      <td>2.2</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.0318</td>\n",
       "      <td>0.87</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0262</td>\n",
       "      <td>0.79</td>\n",
       "      <td>3.9</td>\n",
       "      <td>6.9</td>\n",
       "      <td>1.0314</td>\n",
       "      <td>0.81</td>\n",
       "      <td>2.2</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0246</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.0262</td>\n",
       "      <td>0.79</td>\n",
       "      <td>3.9</td>\n",
       "      <td>6.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-06</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0244</td>\n",
       "      <td>0.85</td>\n",
       "      <td>4.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>1.0246</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        DATE  MONTH  RainToday  RainTomorrow  BASEL_pressure  BASEL_humidity  \\\n",
       "0 2000-01-02      1          0             0          1.0318            0.87   \n",
       "1 2000-01-03      1          0             1          1.0314            0.81   \n",
       "2 2000-01-04      1          1             1          1.0262            0.79   \n",
       "3 2000-01-05      1          1             0          1.0246            0.90   \n",
       "4 2000-01-06      1          0             0          1.0244            0.85   \n",
       "\n",
       "   BASEL_temp_mean  BASEL_sunshine  BASEL_pressure_lag1  BASEL_humidity_lag1  \\\n",
       "0              3.6             0.0               1.0286                 0.89   \n",
       "1              2.2             3.7               1.0318                 0.87   \n",
       "2              3.9             6.9               1.0314                 0.81   \n",
       "3              6.0             3.7               1.0262                 0.79   \n",
       "4              4.2             5.7               1.0246                 0.90   \n",
       "\n",
       "   BASEL_temp_mean_lag1  BASEL_sunshine_lag1  \n",
       "0                   2.9                  0.0  \n",
       "1                   3.6                  0.0  \n",
       "2                   2.2                  3.7  \n",
       "3                   3.9                  6.9  \n",
       "4                   6.0                  3.7  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "PROC_PATH = \"/Users/purvigarg/Downloads/CMSE492/cmse492_project/data/processed/basel_rain_features.csv\"\n",
    "\n",
    "df = pd.read_csv(PROC_PATH)\n",
    "print(\"Loaded processed:\", df.shape)\n",
    "\n",
    "df[\"DATE\"] = pd.to_datetime(df[\"DATE\"].astype(str), errors=\"coerce\")\n",
    "df = df.sort_values(\"DATE\").reset_index(drop=True)\n",
    "\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689ae357",
   "metadata": {},
   "source": [
    "## 2. Define Feature Matrix `X` and Target `y`\n",
    "\n",
    "### 2.1 Use all engineered predictors and the `RainTomorrow` label\n",
    "\n",
    "I now separate the processed table into a feature matrix `X` and the binary\n",
    "target `y = RainTomorrow`. All columns except `DATE` and `RainTomorrow` are used\n",
    "as predictors, including month, current rain flag, and current and lagged Basel\n",
    "weather variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "806988e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns: ['MONTH', 'RainToday', 'BASEL_pressure', 'BASEL_humidity', 'BASEL_temp_mean', 'BASEL_sunshine', 'BASEL_pressure_lag1', 'BASEL_humidity_lag1', 'BASEL_temp_mean_lag1', 'BASEL_sunshine_lag1']\n",
      "X shape: (3653, 10)\n",
      "y distribution:\n",
      "RainTomorrow\n",
      "0    0.532987\n",
      "1    0.467013\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "y = df[\"RainTomorrow\"].astype(int)\n",
    "feature_cols = [c for c in df.columns if c not in [\"DATE\", \"RainTomorrow\"]]\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "\n",
    "print(\"Feature columns:\", feature_cols)\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y distribution:\")\n",
    "print(y.value_counts(normalize=True).rename(\"proportion\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c11fec",
   "metadata": {},
   "source": [
    "\n",
    "The final feature matrix has shape `(3653, 10)` with the columns  \n",
    "`MONTH`, `RainToday`, and eight Basel weather variables (current + lag1).  \n",
    "The `RainTomorrow` label remains well balanced (about 53% no-rain, 47% rain),\n",
    "so both classes are well represented in the training and test sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60338fbb",
   "metadata": {},
   "source": [
    "## 3. Time-Aware Train/Test Split\n",
    "\n",
    "### 3.1 Use an 80/20 chronological split\n",
    "\n",
    "To mimic real forecasting, I train on earlier years and test on later years.\n",
    "Here I take the first 80% of days as the training set and the final 20% of days\n",
    "as the test set, preserving the chronological order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc9e7947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (2922, 10) Test shape: (731, 10)\n",
      "Train dates: 2000-01-02 00:00:00 → 2008-01-01 00:00:00\n",
      "Test dates: 2008-01-02 00:00:00 → 2010-01-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# 4.1 Chronological 80/20 split\n",
    "\n",
    "n = len(df)\n",
    "split_idx = int(0.8 * n)\n",
    "\n",
    "X_train = X.iloc[:split_idx].copy()\n",
    "y_train = y.iloc[:split_idx].copy()\n",
    "\n",
    "X_test = X.iloc[split_idx:].copy()\n",
    "y_test = y.iloc[split_idx:].copy()\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "print(\"Train dates:\", df[\"DATE\"].iloc[0], \"→\", df[\"DATE\"].iloc[split_idx - 1])\n",
    "print(\"Test dates:\", df[\"DATE\"].iloc[split_idx], \"→\", df[\"DATE\"].iloc[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1088cae6",
   "metadata": {},
   "source": [
    "\n",
    "The training set contains 2,922 days and the test set contains 731 days, with\n",
    "train dates from 2000-01-02 to 2008-01-01 and test dates from 2008-01-02 to\n",
    "2010-01-01. This split ensures that the models are always trained on the past\n",
    "and evaluated on a later, unseen period.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76423e20",
   "metadata": {},
   "source": [
    "## 4. Evaluation Helper\n",
    "\n",
    "### 4.1 Shared metric function for all models\n",
    "\n",
    "I define a small helper that reports accuracy, precision, recall, F1 score, and\n",
    "the confusion matrix for any set of predictions. Using the same metrics for both\n",
    "models makes their performance directly comparable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd98f606",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def eval_model(y_true, y_pred, name=\"model\"):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Accuracy :\", f\"{acc:.3f}\")\n",
    "    print(\"Precision:\", f\"{prec:.3f}\")\n",
    "    print(\"Recall   :\", f\"{rec:.3f}\")\n",
    "    print(\"F1       :\", f\"{f1:.3f}\")\n",
    "    print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac936fe1",
   "metadata": {},
   "source": [
    "## 5. Improved Logistic Regression (with Scaling and Class Weights)\n",
    "\n",
    "### 5.1 Pipeline with `StandardScaler` and hyperparameter tuning\n",
    "\n",
    "Here I fit a logistic regression model in a pipeline with feature scaling and a\n",
    "small grid search over the regularization strength `C` and the `class_weight`\n",
    "setting. The search is optimized for F1 on the training folds, emphasizing\n",
    "performance on rainy days.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b33fc2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params (logreg): {'logreg__C': 3.0, 'logreg__class_weight': 'balanced'}\n",
      "\n",
      "=== Improved Logistic Regression ===\n",
      "Accuracy : 0.668\n",
      "Precision: 0.640\n",
      "Recall   : 0.697\n",
      "F1       : 0.668\n",
      "Confusion matrix:\n",
      " [[244 137]\n",
      " [106 244]]\n",
      "\n",
      "Classification report (Improved Logistic Regression):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.697     0.640     0.668       381\n",
      "           1      0.640     0.697     0.668       350\n",
      "\n",
      "    accuracy                          0.668       731\n",
      "   macro avg      0.669     0.669     0.668       731\n",
      "weighted avg      0.670     0.668     0.668       731\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "logreg_pipe = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"logreg\", LogisticRegression(max_iter=2000)),\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"logreg__C\": [0.1, 1.0, 3.0],\n",
    "    \"logreg__class_weight\": [None, \"balanced\"],\n",
    "}\n",
    "\n",
    "logreg_search = GridSearchCV(\n",
    "    logreg_pipe,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"f1\",   # focus on rainy days\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "logreg_search.fit(X_train, y_train)\n",
    "print(\"Best params (logreg):\", logreg_search.best_params_)\n",
    "\n",
    "y_pred_logreg = logreg_search.predict(X_test)\n",
    "logreg_results = eval_model(y_test, y_pred_logreg, name=\"Improved Logistic Regression\")\n",
    "\n",
    "print(\"\\nClassification report (Improved Logistic Regression):\")\n",
    "print(classification_report(y_test, y_pred_logreg, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa3c9d4",
   "metadata": {},
   "source": [
    "\n",
    "The best logistic regression model uses `C = 3.0` with `class_weight = \"balanced\"`.\n",
    "On the 2008–2010 test period it achieves about **66.8% accuracy** and an\n",
    "**F1 score of 0.668** for the rain class, with precision ≈ 0.64 and recall ≈ 0.70.\n",
    "The confusion matrix shows that it detects a large fraction of rainy days while\n",
    "still correctly identifying many dry days, and performance is much stronger and\n",
    "more balanced than the simple logistic baseline from B3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51948cb",
   "metadata": {},
   "source": [
    "## 6. Random Forest Model\n",
    "\n",
    "### 6.1 Nonlinear tree ensemble on the same features\n",
    "\n",
    "Next, I fit a random forest classifier using the same train/test split and\n",
    "features. This model can capture nonlinear relationships between pressure,\n",
    "humidity, temperature, sunshine, and their lags without any extra feature\n",
    "engineering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5721026a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Random Forest ===\n",
      "Accuracy : 0.670\n",
      "Precision: 0.655\n",
      "Recall   : 0.657\n",
      "F1       : 0.656\n",
      "Confusion matrix:\n",
      " [[260 121]\n",
      " [120 230]]\n",
      "\n",
      "Classification report (Random Forest):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.684     0.682     0.683       381\n",
      "           1      0.655     0.657     0.656       350\n",
      "\n",
      "    accuracy                          0.670       731\n",
      "   macro avg      0.670     0.670     0.670       731\n",
      "weighted avg      0.670     0.670     0.670       731\n",
      "\n",
      "\n",
      "Top feature importances (Random Forest):\n",
      "BASEL_pressure          0.174164\n",
      "BASEL_pressure_lag1     0.129110\n",
      "BASEL_temp_mean         0.104143\n",
      "BASEL_temp_mean_lag1    0.101512\n",
      "BASEL_sunshine          0.094483\n",
      "RainToday               0.091315\n",
      "BASEL_humidity_lag1     0.086223\n",
      "BASEL_sunshine_lag1     0.084502\n",
      "BASEL_humidity          0.083258\n",
      "MONTH                   0.051289\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    random_state=0,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "rf_results = eval_model(y_test, y_pred_rf, name=\"Random Forest\")\n",
    "\n",
    "print(\"\\nClassification report (Random Forest):\")\n",
    "print(classification_report(y_test, y_pred_rf, digits=3))\n",
    "\n",
    "# feature importances\n",
    "importances = pd.Series(rf.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "print(\"\\nTop feature importances (Random Forest):\")\n",
    "print(importances.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24bdb5b",
   "metadata": {},
   "source": [
    "\n",
    "The random forest reaches about **67.0% accuracy** and an **F1 score of 0.656**\n",
    "for rainy days, very similar overall performance to the tuned logistic model.\n",
    "Feature importances highlight current and lagged **pressure**, followed by\n",
    "`BASEL_temp_mean`, `BASEL_temp_mean_lag1`, and sunshine-related variables,\n",
    "which matches the earlier physics-based intuition that pressure and recent\n",
    "conditions are strong drivers of rain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8621638",
   "metadata": {},
   "source": [
    "## 7. Compare Improved Models\n",
    "\n",
    "### 7.1 Summary table of logistic regression vs random forest\n",
    "\n",
    "Finally, I gather the main metrics from both improved models into a single\n",
    "summary table and save it for use in the final report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "546bf68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary metrics:\n",
      "                          model  accuracy  precision    recall        f1\n",
      "0  Improved Logistic Regression  0.667579   0.640420  0.697143  0.667579\n",
      "1                 Random Forest  0.670315   0.655271  0.657143  0.656205\n",
      "\n",
      "Saved results table to: /Users/purvigarg/Downloads/CMSE492/cmse492_project/data/processed/improved_model_results.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results_df = pd.DataFrame([logreg_results, rf_results])\n",
    "print(\"Summary metrics:\")\n",
    "print(results_df)\n",
    "\n",
    "results_path = \"/Users/purvigarg/Downloads/CMSE492/cmse492_project/data/processed/improved_model_results.csv\"\n",
    "os.makedirs(os.path.dirname(results_path), exist_ok=True)\n",
    "results_df.to_csv(results_path, index=False)\n",
    "print(\"\\nSaved results table to:\", results_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645110b7",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "This notebook used the processed Basel feature table to train and compare two\n",
    "stronger models for the question “Will it rain tomorrow in Basel?” Using a\n",
    "chronological 80/20 split, both the tuned logistic regression and the random\n",
    "forest reached about 67% accuracy on the 2008–2010 test period. The logistic\n",
    "regression with balanced class weights slightly outperformed the random forest\n",
    "in F1 and recall for rainy days, while remaining easy to interpret and explain.\n",
    "\n",
    "Taken together with the B3 baseline results, these experiments show that\n",
    "careful preprocessing, lagged features, and modest tuning are enough to move\n",
    "from a weak baseline (F1 ≈ 0.53 for Rain) to a stable, interpretable model with\n",
    "F1 ≈ 0.67. In the final report, I treat the tuned logistic regression as the\n",
    "main “final” model and use the random forest as a sanity check that a more\n",
    "flexible method does not provide dramatically better performance on this\n",
    "station-only dataset.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
