{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B3. Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (3654, 165)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# load the same raw file as in B1\n",
    "DATA_PATH = \"/Users/purvigarg/Downloads/CMSE492/cmse492_project/data/raw/weather_prediction_dataset.csv\"\n",
    "data = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(\"Loaded:\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort by Date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"DATE\"] = pd.to_datetime(data[\"DATE\"].astype(str), errors=\"coerce\")\n",
    "data = data.sort_values(\"DATE\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this is weather data, time order really matters. I don’t want to accidentally train on future days and test on past days — that would be leakage. So I converted the DATE column to a real datetime and then I sorted the whole dataframe by date. Now I know that row 0 is an earlier day and the last row is a later day. That makes my train/test split realistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the target (RainTomorrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = \"BASEL\"\n",
    "pref = f\"{city}_\"\n",
    "\n",
    "# label = tomorrow's rain (1 if tomorrow's precipitation > 0)\n",
    "data[\"RainTomorrow\"] = (data[f\"{pref}precipitation\"].shift(-1) > 0).astype(int)\n",
    "data = data.dropna(subset=[\"RainTomorrow\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My actual question is “Will it rain in Basel tomorrow?” but the dataset only tells me whether it rained today. So I made the label myself: I took Basel’s precipitation column and shifted it up by 1 day. That way, today’s row is now paired with tomorrow’s rain (1 or 0). I also dropped the very last row because the last day has no tomorrow. Now I have a proper supervised-learning target called RainTomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick a tiny feature set for the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    f\"{pref}pressure\",\n",
    "    f\"{pref}humidity\",\n",
    "    f\"{pref}temp_mean\",\n",
    "    f\"{pref}snsunshine\".replace(\"sns\",\"sun\") if f\"{pref}snsunshine\".replace(\"sns\",\"sun\") in data.columns else f\"{pref}sunshine\"\n",
    "]\n",
    "# remove columns that don't exist\n",
    "feature_cols = [c for c in feature_cols if c in data.columns]\n",
    "\n",
    "X = data[feature_cols].copy()\n",
    "y = data[\"RainTomorrow\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is only the baseline step, I didn’t want to throw in all 165 columns. I picked the 3–4 Basel features that I already saw in EDA are related to rain: pressure (low → rain), humidity (high → rain), temp_mean (context), and sunshine (low → rain). This gives me a small, clean, and weather-logical feature set. I also filtered the list to avoid errors if one of the columns is missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-ordered train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time–ordered train/test split (first 80% train, last 20% test)\n",
    "split = int(0.8 * len(data))\n",
    "X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
    "y_train, y_test = y.iloc[:split], y.iloc[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to simulate real life: I train on earlier years and test on later years. So I took the first 80% of the rows (earlier dates) as my training set and the last 20% (later dates) as my test set. I did not shuffle because that would mix past and future. This way, the baseline performance I report is honest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline 1 — Majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline 1: Majority (always 0 ) ===\n",
      "Accuracy: 0.521\n",
      "F1 (Rain): 0.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Baseline 1: Majority class (always predict the most common label in TRAIN) \n",
    "majority_label = y_train.mode()[0]\n",
    "y_pred_majority = np.full_like(y_test, fill_value=majority_label)\n",
    "\n",
    "acc_majority = accuracy_score(y_test, y_pred_majority)\n",
    "f1_majority  = f1_score(y_test, y_pred_majority, pos_label=1)\n",
    "\n",
    "print(\"=== Baseline 1: Majority (always\", majority_label, \") ===\")\n",
    "print(f\"Accuracy: {acc_majority:.3f}\")\n",
    "print(f\"F1 (Rain): {f1_majority:.3f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted a “zero-effort” model to compare against. So I built the simplest classifier possible: always predict the class that happens the most in the training data. In my case that’s usually “No Rain.” This baseline tells me what accuracy I get without using any weather information. It also shows me a problem: the F1 score for the Rain class is 0, because this model never predicts rain. That’s good — now I know what I must beat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline 2 — Simple Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline 2: Logistic Regression (simple) ===\n",
      "Confusion matrix:\n",
      "[[234 147]\n",
      " [169 181]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.581     0.614     0.597       381\n",
      "           1      0.552     0.517     0.534       350\n",
      "\n",
      "    accuracy                          0.568       731\n",
      "   macro avg      0.566     0.566     0.565       731\n",
      "weighted avg      0.567     0.568     0.567       731\n",
      "\n",
      "Accuracy: 0.568\n",
      "F1 (Rain): 0.534\n"
     ]
    }
   ],
   "source": [
    "logit = LogisticRegression(max_iter=1000)\n",
    "logit.fit(X_train, y_train)\n",
    "y_pred_logit = logit.predict(X_test)\n",
    "\n",
    "acc_logit = accuracy_score(y_test, y_pred_logit)\n",
    "f1_logit  = f1_score(y_test, y_pred_logit, pos_label=1)\n",
    "\n",
    "print(\"=== Baseline 2: Logistic Regression (simple) ===\")\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_logit))\n",
    "print(classification_report(y_test, y_pred_logit, digits=3))\n",
    "print(f\"Accuracy: {acc_logit:.3f}\")\n",
    "print(f\"F1 (Rain): {f1_logit:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted a real baseline, not just the dumb one. Logistic regression is a good first real model for binary classification — it’s simple, fast, and I can explain it. Here it uses only a few Basel weather features but already predicts both classes. This shows me the problem is actually learnable from the features I chose. Later, when I try trees, random forest, or even a small neural net, I can say “is this better than my logistic?” If it’s not, I know the fancy model isn’t worth it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline results tell me two very different stories. The first model, “always predict No Rain,” got 52.1% accuracy, but that number is a bit fake-helpful — it’s high only because a little over half of the days in this dataset are actually dry. Since it never predicts rain, its F1 for the Rain class is 0.0, which means it completely fails at the part of the problem I actually care about (finding rainy days). So I can already say: accuracy by itself is not enough for this project.\n",
    "\n",
    "When I used a real model — the simple logistic regression — the picture improved. The accuracy went up to 56.8%, so it’s doing better overall than the “always no rain” rule. More importantly, the F1 score for Rain is 0.534, which means the model is now correctly identifying a good chunk of the rainy days and not just the dry ones. The confusion matrix [[234, 147], [169, 181]] tells me how: it correctly said “no rain” 234 times and “rain” 181 times, but it also missed 169 rainy days (false negatives) and raised 147 false alarms (false positives). So the model is useful, but still cautious and a bit noisy. Overall, what I understand from these numbers is: (1) a trivial baseline is not acceptable, (2) even a small, interpretable model can learn signal from Basel weather features, and (3) to get to the ~0.67 accuracy and ~0.68–0.70 F1 we saw later, I need to add better features (lags, seasonality, neighbor pressure) and possibly tune the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
